Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,MeSH Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms,A. Panichella; B. Dit; R. Oliveto; M. Di Penta; D. Poshynanyk; A. De Lucia,"University of Salerno, Fisciano (SA), Italy",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,522,531,"Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606598,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606598,Genetic Algoritms;Latent Dirichlet Allocation;Textual Analysis in Software Engineering,Accuracy;Context;Genetic algorithms;Labeling;Natural languages;Software;Software engineering,genetic algorithms;information retrieval;natural language processing;software engineering;text analysis,IR methods;LDA configurations;LDA-GA;SE tasks;feature location;genetic algorithms;information retrieval methods;latent Dirichlet allocation;natural language documents;natural language text;near-optimal configuration;software artifact labeling;software data;software engineering tasks;software textual retrieval and analysis;source code;topic modeling technique;traceability link recovery,,61,,38,,,,18-26 May 2013,,IEEE,IEEE Conferences
Search-based genetic optimization for deployment and reconfiguration of software in the cloud,S. Frey; F. Fittkau; W. Hasselbring,"Software Engineering Group, Kiel University, 24118 Kiel, Germany",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,512,521,"Migrating existing enterprise software to cloud platforms involves the comparison of competing cloud deployment options (CDOs). A CDO comprises a combination of a specific cloud environment, deployment architecture, and runtime reconfiguration rules for dynamic resource scaling. Our simulator CDOSim can evaluate CDOs, e.g., regarding response times and costs. However, the design space to be searched for well-suited solutions is extremely huge. In this paper, we approach this optimization problem with the novel genetic algorithm CDOXplorer. It uses techniques of the search-based software engineering field and CDOSim to assess the fitness of CDOs. An experimental evaluation that employs, among others, the cloud environments Amazon EC2 and Microsoft Windows Azure, shows that CDOXplorer can find solutions that surpass those of other state-of-the-art techniques by up to 60%. Our experiment code and data and an implementation of CDOXplorer are available as open source software.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606597,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606597,Cloud computing;Deployment optimization;Search-based software engineering,Biological cells;Genetic algorithms;Nickel;Optimization;Sociology;Software;Time factors,cloud computing;genetic algorithms;public domain software;software architecture,Amazon EC2;CDOSim simulator;CDOXplorer;Microsoft Windows Azure;cloud deployment options;cloud environment;cloud platforms;deployment architecture;dynamic resource scaling;enterprise software;genetic algorithm;open source software;optimization problem;runtime reconfiguration rules;search-based genetic optimization;search-based software engineering field;software deployment;software reconfiguration,,26,,34,,,,18-26 May 2013,,IEEE,IEEE Conferences
Hercules: Reproducing Crashes in Real-World Application Binaries,V. T. Pham; W. B. Ng; K. Rubinov; A. Roychoudhury,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,1,,891,901,"Binary analysis is a well-investigated area in software engineering and security. Given real-world program binaries, generating test inputs which cause the binaries to crash is crucial. Generation of crashing inputs has many applications including off-line analysis of software prior to deployment, or online analysis of software patches as they are inserted. In this work, we present a method for generating inputs which reach a given ""potentially crashing"" location. Such potentially crashing locations can be found by a separate static analysis (or by gleaning crash reports submitted by internal / external users) and serve as the input to our method. The test input generated by our method serves as a witness of the crash. Our method is particularly suited for binaries of programs which take in complex structured inputs. Experiments on real-life applications such as the Adobe Reader and the Windows Media Player demonstrate that our Hercules tool built on selective symbolic execution engine S2E can generate crashing inputs within few hours, where symbolic approaches (as embodied by S2E) or blackbox fuzzing approaches (as embodied by the commercial tool PeachFuzzer) failed.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.99,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194635,binary analysis;symbolic execution;test generation,Ash;Computer crashes;Concrete;Heuristic algorithms;Hybrid power systems;Registers;Search problems,security of data;software tools,Adobe Reader;Hercules tool;S2E;Windows Media Player;binary analysis;crash reproduction;selective symbolic execution engine;software engineering;software off-line analysis;software security,,0,,31,,,,16-24 May 2015,,IEEE,IEEE Conferences
Improving feature location practice with multi-faceted interactive exploration,J. Wang; X. Peng; Z. Xing; W. Zhao,"School of Computer Science, Fudan University, Shanghai, China",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,762,771,"Feature location is a human-oriented and information-intensive process. When performing feature location tasks with existing tools, developers often feel it difficult to formulate an accurate feature query (e.g., keywords) and determine the relevance of returned results. In this paper, we propose a feature location approach that supports multi-faceted interactive program exploration. Our approach automatically extracts and mines multiple syntactic and semantic facets from candidate program elements. Furthermore, it allows developers to interactively group, sort, and filter feature location results in a centralized, multi-faceted, and intelligent search User Interface (UI). We have implemented our approach as a web-based tool MFIE and conducted an experimental study. The results show that the developers using MFIE can accomplish their feature location tasks 32% faster and the quality of their feature location results (in terms of F-measure) is 51% higher than that of the developers using regular Eclipse IDE.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606622,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606622,,Educational institutions;Feature extraction;History;Java;Navigation;Semantics;Syntactics,reverse engineering;software maintenance;user interfaces,Web-based tool MFIE;centralized user interface;feature location;feature query;human-oriented process;information-intensive process;intelligent search user interface;multifaceted interactive exploration;multifaceted user interface;semantic facets;syntactic facets,,16,,30,,,,18-26 May 2013,,IEEE,IEEE Conferences
Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis,S. Mechtaev; J. Yi; A. Roychoudhury,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,691,701,"Since debugging is a time-consuming activity, automated program repair tools such as GenProg have garnered interest. A recent study revealed that the majority of GenProg repairs avoid bugs simply by deleting functionality. We found that SPR, a state-of-the-art repair tool proposed in 2015, still deletes functionality in their many ""plausible"" repairs. Unlike generate-and-validate systems such as GenProg and SPR, semantic analysis based repair techniques synthesize a repair based on semantic information of the program. While such semantics-based repair methods show promise in terms of quality of generated repairs, their scalability has been a concern so far. In this paper, we present Angelix, a novel semantics-based repair method that scales up to programs of similar size as are handled by search-based repair tools such as GenProg and SPR. This shows that Angelix is more scalable than previously proposed semantics based repair methods such as SemFix and DirectFix. Furthermore, our repair method can repair multiple buggy locations that are dependent on each other. Such repairs are hard to achieve using SPR and GenProg. In our experiments, Angelix generated repairs from large-scale real-world software such as wireshark and php, and these generated repairs include multi-location repairs. We also report our experience in automatically repairing the well-known Heartbleed vulnerability.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884807,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886945,Automated program repair;Semantic analysis,Computer bugs;Maintenance engineering;Scalability;Semantics;Software;Software engineering;Testing,program debugging;program diagnostics,Angelix method;Heartbleed vulnerability;SPR tool;generate-and-validate systems;multilocation repairs;program debugging;program repair tools;scalable multiline program patch synthesis;semantic analysis;semantic information;symbolic analysis,,17,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
TaskNav: Task-Based Navigation of Software Documentation,C. Treude; M. Sicard; M. Klocke; M. Robillard,"Dept. de Inf. e Mat. Aplic., Univ. Fed. do Rio Grande do Norte, Natal, Brazil",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,2,,649,652,"To help developers navigate documentation, we introduce Task Nav, a tool that automatically discovers and indexes task descriptions in software documentation. With Task Nav, we conceptualize tasks as specific programming actions that have been described in the documentation. Task Nav presents these extracted task descriptions along with concepts, code elements, and section headers in an auto-complete search interface. Our preliminary evaluation indicates that search results identified through extracted task descriptions are more helpful to developers than those found through other means, and that they help bridge the gap between documentation structure and the information needs of software developers. Video: https://www.youtube.com/watch?v=opnGYmMGnqY.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.214,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203034,Auto-Complete;Development Tasks;Natural Language Processing;Navigation;Software Documentation,Documentation;Electronic mail;Instruction sets;Interference;Navigation;Software engineering,document handling;information retrieval;software engineering,TaskNav;auto-complete search interface;code elements;documentation structure;extracted task descriptions;programming actions;section headers;software developers;software documentation;task descriptions;task-based navigation,,3,,16,,,,16-24 May 2015,,IEEE,IEEE Conferences
The Impact of Test Case Summaries on Bug Fixing Performance: An Empirical Investigation,S. Panichella; A. Panichella; M. Beller; A. Zaidman; H. C. Gall,"Univ. of Zurich, Zurich, Switzerland",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,547,558,"Automated test generation tools have been widely investigated with the goal of reducing the cost of testing activities. However, generated tests have been shownnot to help developers in detecting and finding more bugs even though they reach higher structural coverage compared to manual testing. The main reason is that generated tests are difficult to understand and maintain. Our paper proposes an approach, coined TestDescriber, which automatically generates test case summaries of the portion of code exercised by each individual test, thereby improving understandability. We argue that this approach can complement the current techniques around automated unit test generation or search-based techniques designed to generate a possibly minimal set of test cases. In evaluating our approach we found that (1) developers find twice as many bugs, and (2) test case summaries significantly improve the comprehensibility of test cases, which is considered particularly useful by developers.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884847,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886933,Empirical Study;Software testing;Test Case Summarization,Computer bugs;Java;Natural languages;Pragmatics;Software;Software engineering;Testing,program debugging;program testing,TestDescriber approach;automated test generation tools;automated unit test generation;bug fixing performance;search-based techniques;test case summaries,,6,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
"On the ""Naturalness"" of Buggy Code",B. Ray; V. Hellendoorn; S. Godhane; Z. Tu; A. Bacchelli; P. Devanbu,,2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,428,439,"Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be “natural”, like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is “unnatural” in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java projects, and focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for inspection by their average entropy yields cost-effectiveness scores comparable to popular defect prediction methods. At a finer granularity, focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and or- dering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid, simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884848,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886923,,Biological system modeling;Computer bugs;Entropy;Inspection;Predictive models;Software;Standards,program debugging,FindBugs;Java projects;PMD;bug finders;bug fix commits;buggy code naturalness;cost-effectiveness score;entropy measure;language statistics;software naturalness;statistical language model,,4,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
The impact of fault models on software robustness evaluations,S. Winter; C. Sarbu; N. Suri; B. Murphy,"Technischen Universit&#x0E4;t Darmstadt, Darmstadt, Germany",2011 33rd International Conference on Software Engineering (ICSE),20111010,2011,,,51,60,"Following the design and in-lab testing of software, the evaluation of its resilience to actual operational perturbations in the field is a key validation need. Software-implemented fault injection (SWIFI) is a widely used approach for evaluating the robustness of software components. Recent research [24, 18] indicates that the selection of the applied fault model has considerable influence on the results of SWIFI-based evaluations, thereby raising the question how to select appropriate fault models (i.e. that provide justified robustness evidence). This paper proposes several metrics for comparatively evaluating fault models's abilities to reveal robustness vulnerabilities. It demonstrates their application in the context of OS device drivers by investigating the influence (and relative utility) of four commonly used fault models, i.e. bit flips (in function parameters and in binaries), data type dependent parameter corruptions, and parameter fuzzing. We assess the efficiency of these models at detecting robustness vulnerabilities during the SWIFI evaluation of a real embedded operating system kernel and discuss application guidelines for our metrics alongside.",0270-5257;02705257,Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0,10.1145/1985793.1985801,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032444,fault injection;fault models;robustness testing,Complexity theory;Context;Measurement;Robustness;Servers;Software;Transient analysis,operating system kernels;program testing;software fault tolerance;software metrics,OS device driver;SWIFI;bit flips;data type dependent parameter corruption;embedded operating system kernel;fault model;parameter fuzzing;software robustness evaluation;software-implemented fault injection,,7,,32,,,,21-28 May 2011,,IEEE,IEEE Conferences
"Efficient fuzz testing leveraging input, code, and execution",N. Havrikov,"Saarland Univ., Saarbrucken, Germany",2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170824,2017,,,417,420,"Any kind of smart testing technique must be very efficient to be competitive with random fuzz testing. State-of the-art test generators are largely inferior to random testing in real world applications. This work proposes to gather and evaluate lightweight analyses that can enable the creation of an efficient and sufficiently effective analysis-assisted fuzz tester. The analyses shall leverage information sources apart from the program under test itself, such as e.g. descriptions of the targeted input format in the form of extended context-free grammars, or hardware counters. As the main contributions, an efficient framework for building fuzzers around given analyses will be created, and with its help analyses will be identified and categorized according to their performance.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.26,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965373,efficient fuzz testing;fuzz testing;fuzzing;grammar-based testing;software engineering;test input generation,Analytical models;Data models;Generators;Grammar;Measurement;Production;Testing,context-free grammars;program diagnostics;program testing,analysis-assisted fuzz tester;code;execution;extended context-free grammars;hardware counters;input;lightweight analysis;random fuzz testing;smart testing technique;test generators,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
Stride: Search-based deterministic replay in polynomial time via bounded linkage,J. Zhou; X. Xiao; C. Zhang,"The Prism Research Group, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,892,902,"Deterministic replay remains as one of the most effective ways to comprehend concurrent bugs. Existing approaches either maintain the exact shared read-write linkages with a large runtime overhead or use exponential off-line algorithms to search for a feasible interleaved execution. In this paper, we propose Stride, a hybrid solution that records the bounded shared memory access linkages at runtime and infers an equivalent interleaving in polynomial time, under the sequential consistency assumption. The recording scheme eliminates the need for synchronizing the shared read operations, which results in a significant overhead reduction. Comparing to the previous state-of-the-art approach of deterministic replay, Stride reduces, on average, 2.5 times of runtime overhead and produces, on average, 3.88 times smaller logs.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227130,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227130,Concurrency;Debugging;Replaying,Couplings;Instruction sets;Instruments;Law;Runtime;Schedules,polynomials;program debugging;search problems;shared memory systems,Stride;bounded linkage;concurrent bugs;exponential offline algorithms;polynomial time;read write linkages;search based deterministic replay;sequential consistency assumption;shared memory access,,4,,34,,,,2-9 June 2012,,IEEE,IEEE Conferences
Evaluating the specificity of text retrieval queries to support software engineering tasks,S. Haiduc; G. Bavota; R. Oliveto; A. Marcus; A. De Lucia,"Computer Science Department, Wayne State University, Detroit, MI 48202, USA",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,1273,1276,"Text retrieval approaches have been used to address many software engineering tasks. In most cases, their use involves issuing a textual query to retrieve a set of relevant software artifacts from the system. The performance of all these approaches depends on the quality of the given query (i.e., its ability to describe the information need in such a way that the relevant software artifacts are retrieved during the search). Currently, the only way to tell that a query failed to lead to the expected software artifacts is by investing time and effort in analyzing the search results. In addition, it is often very difficult to ascertain what part of the query leads to poor results. We propose a novel pre-retrieval metric, which reflects the quality of a query by measuring the specificity of its terms. We exemplify the use of the new specificity metric on the task of concept location in source code. A preliminary empirical study shows that our metric is a good effort predictor for text retrieval-based concept location, outperforming existing techniques from the field of natural language document retrieval.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227101,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227101,Concept location;Query specificity;Text retrieval,Context;Correlation;Entropy;Information retrieval;Measurement;Natural languages;Software,natural languages;query processing;software metrics;text analysis,natural language text;preretrieval metric;query quality;software artifacts;software engineering tasks;source code;specificity evaluation;specificity metric;text retrieval queries;text retrieval-based concept location,,11,,14,,,,2-9 June 2012,,IEEE,IEEE Conferences
Combining Word2Vec with Revised Vector Space Model for Better Code Retrieval,T. V. Nguyen; A. T. Nguyen; H. D. Phan; T. D. Nguyen; T. N. Nguyen,,2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170703,2017,,,183,185,"API example code search is an important applicationin software engineering. Traditional approaches to API codesearch are based on information retrieval. Recent advance inWord2Vec has been applied to support the retrieval of APIexamples. In this work, we perform a preliminary study thatcombining traditional IR with Word2Vec achieves better retrievalaccuracy. More experiments need to be done to study differenttypes of combination among two lines of approaches.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.90,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965297,Code Search;Information Retrieval;Word2Vec,Computer bugs;Documentation;Java;Search engines;Software engineering;Tutorials,application program interfaces;information retrieval;software engineering;source code (software);vectors,API example code search;Word2Vec;code retrieval;information retrieval;revised vector space model;software engineering,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
Using mutation analysis for a model-clone detector comparison framework,M. Stephan; M. H. Alafi; A. Stevenson; J. R. Cordy,"School of Computing, Queen's University, Kingston, Canada",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1261,1264,"Model-clone detection is a relatively new area and there are a number of different approaches in the literature. As the area continues to mature, it becomes necessary to evaluate and compare these approaches and validate new ones that are introduced. We present a mutation-analysis based model-clone detection framework that attempts to automate and standardize the process of comparing multiple Simulink model-clone detection tools or variations of the same tool. By having such a framework, new research directions in the area of model-clone detection can be facilitated as the framework can be used to validate new techniques as they arise. We begin by presenting challenges unique to model-clone tool comparison including recall calculation, the nature of the clones, and the clone report representation. We propose our framework, which we believe addresses these challenges. This is followed by a presentation of the mutation operators that we plan to inject into our Simulink models that will introduce variations of all the different model clone types that can then be searched for by each respective model-clone detector.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606693,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606693,,Adaptation models;Analytical models;Cloning;Computational modeling;Detectors;Layout;Software packages,software engineering,Simulink model-clone detection tools;clone nature;clone report representation;model clone type search;model-clone detector comparison framework;mutation analysis;mutation operators;recall calculation,,4,,10,,,,18-26 May 2013,,IEEE,IEEE Conferences
EXSYST: Search-based GUI testing,F. Gross; G. Fraser; A. Zeller,"Saarland University Saarbr&#x00FC;cken, Germany",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,1423,1426,"Test generation tools commonly aim to cover structural artefacts of software, such as either the source code or the user interface. However, focusing only on source code can lead to unrealistic or irrelevant test cases, while only exploring a user interface often misses much of the underlying program behavior. Our EXSYST prototype takes a new approach by exploring user interfaces while aiming to maximize code coverage, thus combining the best of both worlds. Experiments show that such an approach can achieve high code coverage matching and exceeding the code coverage of traditional unit-based test generators; yet, by construction every test case is realistic and relevant, and every detected failure can be shown to be caused by a real sequence of input events.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227232,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227232,GUI testing;system testing;test case generation;test coverage,Calculators;Educational institutions;Generators;Graphical user interfaces;Shape;Testing,graphical user interfaces;program testing;system recovery,EXSYST;code coverage matching;failure detection;program behavior;search-based GUI testing;software structural artefact;source code;system testing;test case generation;test coverage;test generation tool;unit-based test generator;user interface,,9,,10,,,,2-9 June 2012,,IEEE,IEEE Conferences
Combining Symbolic Execution and Model Checking for Data Flow Testing,T. Su; Z. Fu; G. Pu; J. He; Z. Su,"Shanghai Key Lab. of Trustworthy Comput., East China Normal Univ., Shanghai, China",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,1,,654,665,"Data flow testing (DFT) focuses on the flow of data through a program. Despite its higher fault-detection ability over other structural testing techniques, practical DFT remains a significant challenge. This paper tackles this challenge by introducing a hybrid DFT framework: (1) The core of our framework is based on dynamic symbolic execution (DSE), enhanced with a novel guided path search to improve testing performance, and (2) we systematically cast the DFT problem as reach ability checking in software model checking to complement our DSE-based approach, yielding a practical hybrid DFT technique that combines the two approaches' respective strengths. Evaluated on both open source and industrial programs, our DSE-based approach improves DFT performance by 60~80% in terms of testing time compared with state-of-the-art search strategies, while our combined technique further reduces 40% testing time and improves data-flow coverage by 20% by eliminating infeasible test objectives. This combined approach also enables the cross-checking of each component for reliable and robust testing results.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.81,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194614,coverage criteria;data flow coverage;data flow testing;model checking;symbolic execution,Discrete Fourier transforms;Engines;Model checking;Safety;Search problems;Software,data flow computing;formal verification;program testing;public domain software;software fault tolerance;software performance evaluation,DSE;data flow testing;dynamic symbolic execution;fault-detection ability;guided path search;hybrid DFT framework;industrial program;open source program;performance testing;software model checking;structural testing,,4,,61,,,,16-24 May 2015,,IEEE,IEEE Conferences
Synthesizing API usage examples,R. P. L. Buse; W. Weimer,"Department of Computer Science, University of Virginia, Charlottesville, VA, USA",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,782,792,"Key program interfaces are sometimes documented with usage examples: concrete code snippets that characterize common use cases for a particular data type. While such documentation is known to be of great utility, it is burdensome to create and can be incomplete, out of date, or not representative of actual practice. We present an automatic technique for mining and synthesizing succinct and representative human-readable documentation of program interfaces. Our algorithm is based on a combination of path sensitive dataflow analysis, clustering, and pattern abstraction. It produces output in the form of well-typed program snippets which document initialization, method calls, assignments, looping constructs, and exception handling. In a human study involving over 150 participants, 82% of our generated examples were found to be at least as good at human-written instances and 94% were strictly preferred to state of the art code search.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227140,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227140,,Abstracts;Algorithm design and analysis;Clustering algorithms;Concrete;Documentation;Humans;Java,application program interfaces;data flow analysis;data mining;document handling;exception handling;pattern clustering,API usage example synthesis;concrete code snippets;document initialization;exception handling;key program interfaces;looping constructs;method assignment;method calls;path sensitive dataflow analysis;pattern abstraction;pattern clustering;program interface documentation mining;program interface documentation synthesis;program snippets,,34,1,39,,,,2-9 June 2012,,IEEE,IEEE Conferences
SWIM: Synthesizing What I Mean - Code Search and Idiomatic Snippet Synthesis,M. Raghothaman; Y. Wei; Y. Hamadi,"Univ. of Pennsylvania, Philadelphia, PA, USA",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,357,367,"Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe SWIM, a tool which suggests code snippets given API-related natural language queries such as “generate md5 hash code”. The query does not need to contain framework-specific trivia such as the type names or methods of interest. We translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce structured call sequences to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis. We evaluated SWIM with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884808,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886917,Free form queries;code search;idiomatic snippet synthesis;structured call sequences,C# languages;Data models;Libraries;Natural languages;Pattern matching;Search engines;Web pages,application program interfaces;query processing;search engines,API usage patterns;API-related natural language queries;Bing;Google;SWIM framework;application program interface;code search;idiomatic snippet synthesis;method call sequences;programming framework;search engines;synthesizing what i mean,,3,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
1st International workshop on combining modelling and search-based software engineering (CMSBSE 2013),M. Harman; R. F. Paige; J. R. Williams,"CREST Centre, University College London, Malet Place, London, WC1E 6BT, U.K.",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1513,1514,"Modelling plays a vital and pervasive role in software engineering: it provides means to manage complexity via abstraction, and enables the creation of larger, more complex systems. Search-based software engineering (SBSE) offers a productive and proven approach to software engineering through automated discovery of near-optimal solutions to problems, and has proven itself to be effective on a wide variety of software-and systems engineering problems. CMSBSE 2013 was a forum allowing researchers from both communities to meet, discuss synergies and differences, and present topics related to the intersection of search and modelling. Particular goals of CMSBSE were to highlight that SBSE and modelling have substantial conceptual and technical synergy, and to identify and present opportunities in which they can be combined, whilst also aiming to grow the community working in this area.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606763,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606763,Modelling;Search-based software engineering,Communities;Conferences;Educational institutions;Modeling;Search problems;Software;Software engineering,,,,0,,14,,,,18-26 May 2013,,IEEE,IEEE Conferences
How and When to Transfer Software Engineering Research via Extensions,D. Shepherd; K. Damevski; L. Pollock,"ABB Corp. Res., Raleigh, NC, USA",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,2,,239,240,"It is often reported that there is a large gap between software engineering research and practice, with little transfer from research to practice. While this is true in general, one transfer technique is increasingly breaking down this barrier: extensions to integrated development environments (IDEs). With the proliferation of app stores for IDEs and increasing transfer effort from researchers several research-based extensions have seen significant adoption. In this talk we'll discuss our experience transferring code search research, which currently is in the top 5% of Visual Studio extensions with over 13,000 downloads, as well as other research techniques transferred via extensions such as NCrunch, FindBugs, Code Recommenders, Mylyn, and Instasearch. We'll use the lessons learned from our transfer experience to provide case study evidence as to best practices for successful transfer, supplementing it with the quantitative evidence offered by app store and usage data across the broader set of extensions. The goal of this 30 minute talk is to provide researchers with a realistic view on which research techniques can be transferred to practice as well as concrete steps to execute such a transfer.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.152,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202968,case study;integrated development environment;plugins;technology transfer,Computer bugs;Electronic mail;Prototypes;Software;Software engineering;Testing;Visualization,software engineering,IDE;Visual Studio extensions;integrated development environments;research-based extensions;software engineering research,,1,,14,,,,16-24 May 2015,,IEEE,IEEE Conferences
On the value of user preferences in search-based software engineering: A case study in software product lines,A. S. Sayyad; T. Menzies; H. Ammar,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,492,501,"Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606595,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606595,Feature Models;Indicator-Based Evolutionary Algorithm;Multiobjective Optimization;Optimal Feature Selection;Search-Based Software Engineering;Software Product Lines,Evolutionary computation;Mobile handsets;Optimization;Sociology;Software;Software algorithms;Software engineering,evolutionary computation;software engineering,IBEA;NSGA-II;SPEA2;decision spaces;indicator-based evolutionary algorithm;search-based software engineering methods;software design;software product lines;user preferences,,42,,35,,,,18-26 May 2013,,IEEE,IEEE Conferences
Combining Multi-Objective Search and Constraint Solving for Configuring Large Software Product Lines,C. Henard; M. Papadakis; M. Harman; Y. Le Traon,"Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg, Luxembourg",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,1,,517,528,"Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p <; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (Ȃ<sub>12</sub> = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.69,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194602,,Filtering algorithms;Frequency modulation;Measurement;Optimization;Search problems;Software;Software product lines,configuration management;optimisation;search problems;software metrics;software product lines,SATIBEA framework;SPL feature selection;constraint solving;diversity metrics;multiobjective search-based optimization;software product lines configuration,,17,,55,,,,16-24 May 2015,,IEEE,IEEE Conferences
Guiding Dynamic Symbolic Execution toward Unverified Program Executions,M. Christakis; P. Müller; V. Wüstholz,"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,144,155,"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884843,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899,dynamic symbolic execution;partial verification;program verification;static analysis;testing,Aerospace electronics;Conferences;Instruments;Performance analysis;Redundancy;Software engineering;Testing,program testing,Clousot;Pex tool;arithmetic overflow;automatic test case generation;code instrumentation;dynamic symbolic execution;program error detection;program execution;unverified program execution,,,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
Studying Multi-threaded Behavior with TSViz,M. Nunes; H. Lalh; A. Sharma; A. Wong; S. Miucin; A. Fedorova; I. Beschastnikh,"Comput. Sci., Univ. Fed. de Minas Gerais, Belo Horizonte, Brazil",2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170703,2017,,,35,38,"Modern high-performing systems make extensive use of multiple CPU cores. These multi-threaded systems are complex to design, build, and understand. Debugging performance of these multi-threaded systems is especially challenging. This requires the developer to understand the relative execution of dozens of threads and their inter-dependencies, including data-sharing and synchronization behaviors. We describe TSViz, a visualization tool to help developers study and understand the activity of complex multi-threaded systems. TSviz depicts the partial order of concurrent events in a time-space diagram, and simultaneously scales this diagram according to the physical clock timestamps that tag each event. A developer can then interact with the visualization in several ways, for example by searching for events of interest, studying the distribution of critical sections across threads and zooming the diagram in and out. We overview TSviz design and describe our experience with using it to study a high-performance multi-threaded key-value store based on MongoDB. A video demo of TSViz is online: https://youtu.be/LpuiOZ3PJCk.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.9,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965251,concurrency;log analysis;multi-threaded systems;performance debugging;tracing;visualization,Clocks;Debugging;Instruction sets;Synchronization;Tools;Visualization,multi-threading;system monitoring,MongoDB;TSViz;high-performance multithreaded key-value store;multithreaded systems;time-space diagram;visualization tool,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
Billions and billions of constraints: Whitebox fuzz testing in production,E. Bounimova; P. Godefroid; D. Molnar,"Microsoft Research, USA",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,122,131,"We report experiences with constraint-based whitebox fuzz testing in production across hundreds of large Windows applications and over 500 machine years of computation from 2007 to 2013. Whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program. These inputs execute previously uncovered paths or trigger security vulnerabilities. Whitebox fuzzing has found one-third of all file fuzzing bugs during the development of Windows 7, saving millions of dollars in potential security vulnerabilities. The technique is in use today across multiple products at Microsoft. We describe key challenges with running whitebox fuzzing in production. We give principles for addressing these challenges and describe two new systems built from these principles: SAGAN, which collects data from every fuzzing run for further analysis, and JobCenter, which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines. Since June 2010, SAGAN has logged over 3.4 billion constraints solved, millions of symbolic executions, and tens of millions of test cases generated. Our work represents the largest scale deployment of whitebox fuzzing to date, including the largest usage ever for a Satisfiability Modulo Theories (SMT) solver. We present specific data analyses that improved our production use of whitebox fuzzing. Finally we report data on the performance of constraint solving and dynamic test generation that points toward future research problems.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606558,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606558,,Computer bugs;Monitoring;Production;Security;Servers;Testing,computability;data analysis;program diagnostics;program testing;security of data;virtual machines,JobCenter;SAGAN system;SMT solver;Windows 7;binary traces;commodity virtual machines;constraint solving performance;constraint-based whitebox fuzz testing;data analysis;dynamic test generation;program testing;satisfiability modulo theories;security vulnerability;symbolic execution,,21,,23,,,,18-26 May 2013,,IEEE,IEEE Conferences
Chiminey: Reliable Computing and Data Management Platform in the Cloud,I. I. Yusuf; I. E. Thomas; M. Spichkova; S. Androulakis; G. R. Meyer; D. W. Drumm; G. Opletal; S. P. Russo; A. M. Buckle; H. W. Schmidt,"Appl. Data Sci., Australia",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,2,,677,680,"The enabling of scientific experiments that are embarrassingly parallel, long running and data-intensive into a cloud-based execution environment is a desirable, though complex undertaking for many researchers. The management of such virtual environments is cumbersome and not necessarily within the core skill set for scientists and engineers. We present here Chiminey, a software platform that enables researchers to (i) run applications on both traditional high-performance computing and cloud-based computing infrastructures, (ii) handle failure during execution, (iii) curate and visualise execution outputs, (iv) share such data with collaborators or the public, and (v) search for publicly available data.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.221,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203041,Cloud computing;HPC;data curation;data discovery;data management;data publication;fault tolerance;reliability;visualisation,Cloud computing;Connectors;Data visualization;Fault tolerance;Fault tolerant systems;Physics,cloud computing;data handling;parallel processing;software reliability;virtualisation,Chiminey;cloud-based computing infrastructure;data management platform;high-performance computing;software platform;software reliability;virtual environment,,3,,13,,,,16-24 May 2015,,IEEE,IEEE Conferences
Floating-Point Precision Tuning Using Blame Analysis,C. Rubio-González; C. Nguyen; B. Mehne; K. Sen; J. Demmel; W. Kahan; C. Iancu; W. Lavrijsen; D. H. Bailey; D. Hough,"Univ. of California, Davis, Davis, CA, USA",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,1074,1085,"While tremendously useful, automated techniques for tuning the precision of floating-point programs face important scalability challenges. We present Blame Analysis, a novel dynamic approach that speeds up precision tuning. Blame Analysis performs floating-point instructions using different levels of accuracy for their operands. The analysis determines the precision of all operands such that a given precision is achieved in the final result of the program. Our evaluation on ten scientific programs shows that Blame Analysis is successful in lowering operand precision. As it executes the program only once, the analysis is particularly useful when targeting reductions in execution time. In such case, the analysis needs to be combined with search-based tools such as Precimonious. Our experiments show that combining Blame Analysis with Precimonious leads to obtaining better results with significant reduction in analysis time: the optimized programs execute faster (in three cases, we observe as high as 39.9% program speedup) and the combined analysis time is 9× faster on average, and up to 38× faster than Precimonious alone.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884850,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886981,floating point;mixed precision;program optimization,Government;Numerical analysis;Performance analysis;Scalability;Search problems;Software;Tuning,program diagnostics,Precimonious tool;blame analysis approach;floating-point instructions;floating-point precision tuning;operand precision,,2,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
Automatic detection of performance deviations in the load testing of Large Scale Systems,H. Malik; H. Hemmati; A. E. Hassan,"Software Analysis and Intelligence Lab (SAIL) School of Computing, Queen's University, Kingston, Canada",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1012,1021,"Load testing is one of the means for evaluating the performance of Large Scale Systems (LSS). At the end of a load test, performance analysts must analyze thousands of performance counters from hundreds of machines under test. These performance counters are measures of run-time system properties such as CPU utilization, Disk I/O, memory consumption, and network traffic. Analysts observe counters to find out if the system is meeting its Service Level Agreements (SLAs). In this paper, we present and evaluate one supervised and three unsupervised approaches to help performance analysts to 1) more effectively compare load tests in order to detect performance deviations which may lead to SLA violations, and 2) to provide them with a smaller and manageable set of important performance counters to assist in root-cause analysis of the detected deviations. Our case study is based on load test data obtained from both a large scale industrial system and an open source benchmark application. The case study shows, that our wrapper-based supervised approach, which uses a search-based technique to find the best subset of performance counters and a logistic regression model for deviation prediction, can provide up to 89% reduction in the set of performance counters while detecting performance deviations with few false positives (i.e., 95% average precision). The study also shows that the supervised approach is more stable and effective than the unsupervised approaches but it has more overhead due to its semi-automated training phase.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606651,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606651,Machine Learning;Performance;Signature,Control charts;Large-scale systems;Logistics;Monitoring;Principal component analysis;Radiation detectors;Testing,input-output programs;program testing;public domain software;regression analysis;software performance evaluation;unsupervised learning,CPU utilization;LSS;SLA violations;automatic performance deviation detection;deviation prediction;disk I-O;large scale systems;load testing;logistic regression model;machine learning;memory consumption;network traffic;open source benchmark application;performance counters;root-cause analysis;run-time system properties;search-based technique;service level agreements;wrapper-based supervised approach,,24,,18,,,,18-26 May 2013,,IEEE,IEEE Conferences
SNIPR: Complementing code search with code retargeting capabilities,H. A. Sanchez,"Computer Science Department, University of California, Santa Cruz, Santa Cruz, CA 95060",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1423,1426,"This paper sketches a research path that seeks to examine the search for suitable code problem, based on the observation that when code retargeting is included within a code search activity, developers can justify the suitability of these results upfront and thus reduce their searching efforts looking for suitable code. To support this observation, this paper introduces the Snippet Retargeting Approach, or simply SNIPR. SNIPR complements code search with code retargeting capabilities. These capabilities' intent is to help expedite the process of determining if a found example is a best fit. They do that by allowing developers to explore code modification ideas in place, without requiring to leave the search interface. With SNIPR, developers engage in a virtuous loop where they find code, retarget code, and select only code choices they can justify as suitable. This assures immediate feedback on retargeted examples and thus saves valuable time searching for appropriate code.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606733,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606733,,Conferences;Presses;Programming;Prototypes;Search engines;Search problems;Software engineering,program compilers,SNIPR;code problem;code retargeting capabilities;code search activity;complementing code search;research path;snippet retargeting approach,,1,,27,,,,18-26 May 2013,,IEEE,IEEE Conferences
"Graph-based pattern-oriented, context-sensitive source code completion",A. T. Nguyen; T. T. Nguyen; H. A. Nguyen; A. Tamrawi; H. V. Nguyen; J. Al-Kofahi; T. N. Nguyen,"Electrical and Computer Engineering Department, Iowa State University",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,69,79,"Code completion helps improve developers' programming productivity. However, the current support for code completion is limited to context-free code templates or a single method call of the variable on focus. Using software libraries for development, developers often repeat API usages for certain tasks. Thus, a code completion tool could make use of API usage patterns. In this paper, we introduce GraPacc, a graph-based, pattern-oriented, context-sensitive code completion approach that is based on a database of such patterns. GraPacc represents and manages the API usage patterns of multiple variables, methods, and control structures via graph-based models. It extracts the context-sensitive features from the code under editing, e.g. the API elements on focus and their relations to other code elements. Those features are used to search and rank the patterns that are most fitted with the current code. When a pattern is selected, the current code will be completed via a novel graph-based code completion algorithm. Empirical evaluation on several real-world systems shows that GraPacc has a high level of accuracy in code completion.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227205,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227205,API usage pattern;pattern-based code completion,Context;Databases;Feature extraction;Graphical user interfaces;Layout;Libraries;Pattern matching,application program interfaces;feature extraction;graph theory;software libraries;source coding,API elements;API usage patterns;GraPacc;code elements;context-free code templates;context-sensitive feature extraction;developer programming productivity improvement;graph-based pattern-oriented context-sensitive source code completion;single method call;software libraries,,28,1,36,,,,2-9 June 2012,,IEEE,IEEE Conferences
Where does this code come from and where does it go? — Integrated code history tracker for open source systems,K. Inoue; Y. Sasaki; P. Xia; Y. Manabe,"Osaka University, Osaka, Japan",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,331,341,"When we reuse a code fragment in an open source system, it is very important to know the history of the code, such as the code origin and evolution. In this paper, we propose an integrated approach to code history tracking for open source repositories. This approach takes a query code fragment as its input, and returns the code fragments containing the code clones with the query code. It utilizes publicly available code search engines as external resources. Based on this model, we have designed and implemented a prototype system named Ichi Tracker. Using Ichi Tracker, we have conducted three case studies. These case studies show the ancestors and descendents of the code, and we can recognize their evolution history.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227181,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227181,Code Search;Open Source System;Software Evolution,Cloning;Engines;Google;History;Licenses;Search engines;Strontium,public domain software;search engines;software maintenance;software reusability,Ichi Tracker;code ancestors;code clones;code descendents;code evolution;code origin;integrated code history tracker;open source repositories;open source systems;publicly available code search engines;query code fragment reusability,,6,,37,,,,2-9 June 2012,,IEEE,IEEE Conferences
Search-Based Migration of Model Variants to Software Product Line Architectures,W. K. G. Assunção,"DINF, Fed. Univ. of Parana, Curitiba, Brazil",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,2,,895,898,"Software Product Lines (SPLs) are families of related software systems developed for specific market segments or domains. Commonly, SPLs emerge from sets of existing variants when their individual maintenance becomes infeasible. However, current approaches for SPL migration do not support design models, are partially automated, or do not reflect constraints from SPL domains. To tackle these limitations, the goal of this doctoral research plan is to propose an automated approach to the SPL migration process at the design level. This approach consists of three phases: detection, analysis and transformation. It uses as input the class diagrams and lists of features for each system variant, and relies on search-based algorithms to create a product line architecture that best captures the variability present in the variants. Our expected contribution is to support the adoption of SPL practices in companies that face the scenario of migrating variants to SPLs.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.286,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203108,Migration;Re-engineering;Reuse;Search-Based Software Engineering;Software Product Line,Feature extraction;Medical services;Programmable logic arrays;Software;Software product lines;Unified modeling language,software architecture;software product lines,SPL architecture;SPL migration process;product line architecture;search-based algorithm;search-based migration algorithm;software product line architecture,,0,,24,,,,16-24 May 2015,,IEEE,IEEE Conferences
Local Analysis for Global Inputs,A. Kampmann,"Saarland Univ., Saarbrucken, Germany",2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170703,2017,,,433,436,"Fuzz testing and symbolic test generation both face their own challenges. While symbolic testing has scalability issues, fuzzing cannot uncover faults which require carefully engineered inputs. In this paper I propose a combination of both approaches, compensating weaknesses of each approach with the strength of the other approach. I present my plans for evaluation, which include applications of the hybrid tool to programs which neither of the approaches can handle on its own.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.32,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965377,fuzzing;hybrid;symbolic execution;system testing;test generation;unit testing,Computer bugs;Concrete;Grammar;Software;Telemetry;Testing;Tools,program testing,fuzz testing;global inputs;hybrid tool;local analysis;scalability issues;symbolic test generation,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
Memoise: A tool for memoized symbolic execution,G. Yang; S. Khurshid; C. S. Păsăreanu,"Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712, USA",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1343,1346,"This tool paper presents a tool for performing memoized symbolic execution (Memoise), an approach we developed in previous work for more efficient application of symbolic execution. The key idea in Memoise is to allow re-use of symbolic execution results across different runs of symbolic execution without having to re-compute previously computed results as done in earlier approaches. Specifically, Memoise builds a trie-based data structure to record path exploration information during a run of symbolic execution, optimizes the trie for the next run, and re-uses the resulting trie during the next run. Our tool optimizes symbolic execution in three standard scenarios where it is commonly applied: iterative deepening, regression analysis, and heuristic search. Our tool Memoise builds on the Symbolic PathFinder framework to provide more efficient symbolic execution of Java programs and is available online for download. The tool demonstration video is available at http://www.youtube.com/watch?v=ppfYOB0Z2vY.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606713,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606713,,Data structures;Iterative methods;Java;NASA;Regression analysis;Standards;Testing,Java;data structures;iterative methods;program verification;regression analysis,Java programs;Memoise;heuristic search;iterative deepening;memoized symbolic execution;path exploration information;regression analysis;symbolic PathFinder framework;tool demonstration video;trie-based data structure,,4,,13,,,,18-26 May 2013,,IEEE,IEEE Conferences
Information foraging as a foundation for code navigation: NIER track,N. Niu; A. Mahmoud; G. Bradshaw,"Mississippi State University, Mississippi State, MS, USA",2011 33rd International Conference on Software Engineering (ICSE),20111010,2011,,,816,819,A major software engineering challenge is to understand the fundamental mechanisms that underlie the developer's code navigation behavior. We propose a novel and unified theory based on the premise that we can study developer's information seeking strategies in light of the foraging principles that evolved to help our animal ancestors to find food. Our preliminary study on code navigation graphs suggests that the tenets of information foraging provide valuable insight into software maintenance. Our research opens the avenue towards the development of ecologically valid tool support to augment developers' code search skills.,0270-5257;02705257,Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0,10.1145/1985793.1985911,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032526,foraging theory;program comprehension;software maintenance,Biological system modeling;Debugging;Maintenance engineering;Navigation;Profitability;Software engineering;Software maintenance,software maintenance,NIER track;code navigation;foundation;information foraging;information seeking;software engineering,,5,,15,,,,21-28 May 2011,,IEEE,IEEE Conferences
Fast and Precise Statistical Code Completion,P. Roos,"ETH Zurich, Zurich, Switzerland",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,2,,757,759,The main problem we try to solve is API code completion which is both precise and works in real-time. We describe an efficient implementation of an N-gram language model combined with several smoothing methods and a completion algorithm based on beam search. We show that our system is both fast and precise using a thorough experimental evaluation. With optimal parameters we are able to find completions in milliseconds and the desired completion is in the top 3 suggestions in 89% of the time.,0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.240,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203061,APIs;Code Completion;Statistical Language Models;Synthesis,Computational modeling;Libraries;Probability;Real-time systems;Runtime;Semantics;Smoothing methods,application program interfaces;programming languages;statistical analysis,API code completion;N-gram language model;beam search;completion algorithm;experimental evaluation;smoothing methods;statistical code completion,,0,,16,,,,16-24 May 2015,,IEEE,IEEE Conferences
On the relationships between domain-based coupling and code clones: An exploratory study,M. S. Rahman; A. Aryani; C. K. Roy; F. Perin,"University of Saskatchewan, Canada",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1265,1268,"Knowledge of similar code fragments, also known as code clones, is important to many software maintenance activities including bug fixing, refactoring, impact analysis and program comprehension. While a great deal of research has been conducted for finding techniques and implementing tools to identify code clones, little research has been done to analyze the relationships between code clones and other aspects of software. In this paper, we attempt to uncover the relationships between code clones and coupling among domain-level components. We report on a case study of a large-scale open source enterprise system, where we demonstrate that the probability of finding code clones among components with domain-based coupling is more than 90%. While such a probabilistic view does not replace a clone detection tool per se, it certainly has the potential to complement the existing tools by providing the probability of having code clones between software components. For example, it can both reduce the clone search space and provide a flexible and language independent way of focusing only on a specific part of the system. It can also provide a higher level of abstraction to look at the cloning relationships among software components.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606694,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606694,,Cloning;Computer bugs;Couplings;Maintenance engineering;Manuals;Software;User interfaces,program debugging;security of data;software maintenance,bug fixing;clone detection tool;cloning relationships;code clones;code fragments;domain based coupling;domain level components;impact analysis;open source enterprise system;program comprehension;refactoring;software components;software maintenance activities,,3,1,12,,,,18-26 May 2013,,IEEE,IEEE Conferences
Assisting developers of Big Data Analytics Applications when deploying on Hadoop clouds,W. Shang; Z. M. Jiang; H. Hemmati; B. Adams; A. E. Hassan; P. Martin,"Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, Canada",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,402,411,"Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606586,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606586,Big-Data Analytics Application;Cloud Computing;Hadoop;Log Analysis;Monitoring and Debugging,Context;Data handling;Data storage systems;Information management;Joining processes;Keyword search;Programming,cloud computing;data analysis;formal verification;parallel processing;program debugging;public domain software;software fault tolerance;system monitoring,Hadoop clouds;Hadoop-based BDA Apps;big data analysis;big data analytics applications;cloud deployments;deployment verification effort reduction;developer assistance;execution log abstraction;execution sequence recovery;parallel processing frameworks;software applications,,25,,26,,,,18-26 May 2013,,IEEE,IEEE Conferences
A Genetic Algorithm for Detecting Significant Floating-Point Inaccuracies,D. Zou; R. Wang; Y. Xiong; L. Zhang; Z. Su; H. Mei,"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,1,,529,539,"It is well-known that using floating-point numbers may inevitably result in inaccurate results and sometimes even cause serious software failures. Safety-critical software often has strict requirements on the upper bound of inaccuracy, and a crucial task in testing is to check whether significant inaccuracies may be produced. The main existing approach to the floating-point inaccuracy problem is error analysis, which produces an upper bound of inaccuracies that may occur. However, a high upper bound does not guarantee the existence of inaccuracy defects, nor does it give developers any concrete test inputs for debugging. In this paper, we propose the first metaheuristic search-based approach to automatically generating test inputs that aim to trigger significant inaccuracies in floating-point programs. Our approach is based on the following two insights: (1) with FPDebug, a recently proposed dynamic analysis approach, we can build a reliable fitness function to guide the search; (2) two main factors - the scales of exponents and the bit formations of significands - may have significant impact on the accuracy of the output, but in largely different ways. We have implemented and evaluated our approach over 154 real-world floating-point functions. The results show that our approach can detect significant inaccuracies in the subjects.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.70,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194603,,Accuracy;Algorithm design and analysis;Genetic algorithms;Search problems;Sociology;Software;Statistics,floating point arithmetic;genetic algorithms;program debugging;search problems;system monitoring,FPDebug;debugging;dynamic analysis approach;error analysis;fitness function;floating-point inaccuracy detection;floating-point inaccuracy problem;floating-point numbers;floating-point programs;genetic algorithm;metaheuristic search-based approach;safety-critical software;software failures,,7,,34,,,,16-24 May 2015,,IEEE,IEEE Conferences
Search based design of software product lines architectures,T. E. Colanzi,"Computer Science Department, Federal University of Paran&#x00E1;, UFPR, Brazil",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,1507,1510,"The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). However, obtaining a modular, extensible and reusable PLA is a people-intensive and non-trivial task, related to different and possible conflicting factors. Hence, the PLA design is a hard problem and to find the best architecture can be formulated as an optimization problem with many factors. Similar Software Engineering problems have been efficiently solved by search-based algorithms in the field known as Search-based Software Engineering. The existing approaches used to optimize software architecture are not suitable since they do not encompass specific characteristics of SPL. To easy the SPL development and to automate the PLA design this work introduces a multi-objective optimization approach to the PLA design. The approach is now being implemented by using evolutionary algorithms. Empirical studies will be performed to validate the neighborhood operators, SPL measures and search algorithms chosen. Finally, we intend to compare the results of the proposed approach with PLAs designed by human architects.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227050,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227050,multi-objective algorithms;software architecture optimization;software product lines,Computer architecture;Optimization;Programmable logic arrays;Search problems;Software;Software architecture,evolutionary computation;product development;search problems;software architecture;software reusability,PLA design;SPL;evolutionary algorithms;multi objective optimization approach;neighborhood operators;search based design;search-based algorithms;search-based software engineering;software architecture;software product lines architectures,,3,,23,,,,2-9 June 2012,,IEEE,IEEE Conferences
A practical guide for using statistical tests to assess randomized algorithms in software engineering,A. Arcuri; L. Briand,"Simula Research Laboratory, Lysaker, Norway",2011 33rd International Conference on Software Engineering (ICSE),20111010,2011,,,1,10,"Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering.",0270-5257;02705257,Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0,10.1145/1985793.1985795,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032439,bonferroni adjustment;confidence interval;effect size;non-parametric test;parametric test;statistical difference;survey;systematic review,Algorithm design and analysis;Context;Search problems;Software algorithms;Software engineering;Statistical analysis;Testing,software engineering;statistical analysis,practical guide;randomized algorithms;snapshot representation;software engineering;statistical tests,,97,,60,,,,21-28 May 2011,,IEEE,IEEE Conferences
Query quality prediction and reformulation for source code search: The Refoqus tool,S. Haiduc; G. De Rosa; G. Bavota; R. Oliveto; A. de Lucia; A. Marcus,"Department of Computer Science, Wayne State University, Detroit, MI 48202, USA",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1307,1310,"Developers search source code frequently during their daily tasks, to find pieces of code to reuse, to find where to implement changes, etc. Code search based on text retrieval (TR) techniques has been widely used in the software engineering community during the past decade. The accuracy of the TR-based search results depends largely on the quality of the query used. We introduce Refoqus, an Eclipse plugin which is able to automatically detect the quality of a text retrieval query and to propose reformulations for it, when needed, in order to improve the results of TR-based code search. A video of Refoqus is found online at http://www.youtube.com/watch?v=UQlWGiauyk4.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606704,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606704,Query Quality;Query Reformulation;Source Code Search;Text Retrieval,Context;Feature extraction;Software engineering;Software maintenance;Software systems;Training;Training data,query formulation;software engineering,Eclipse plugin;Refoqus tool;TR-based search;query quality prediction;query reformulation;software engineering;source code search;text retrieval query;text retrieval technique,,2,,11,,,,18-26 May 2013,,IEEE,IEEE Conferences
Mining Input Grammars with AUTOGRAM,M. Hoschele; A. Zeller,"Saarland Inf. Campus, Saarland Univ., Saarbrucken, Germany",2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170824,2017,,,31,34,"Knowledge about how a program processes its inputs can help to understand the structure of the input as well as the structure of the program. In a JSON value like [1, true, ""Alice""], for instance the integer value 1, the boolean value true and the string value ""Alice"" would be handled by different functions or stored in different variables. Our AUTOGRAM tool uses dynamic tainting to trace the data flow of each input character for a set of sample inputs and identifies syntactical entities by grouping input fragments that are handled by the same functions. The resulting context-free grammar reflects the structure of valid inputs and can be used for reverse engineering of formats and can serve as direct input for test generators. A video demonstrating AUTOGRAM is available at https://youtu.be/Iqym60iWBBk.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.14,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965250,Input formats;context-free grammars;dynamic tainting;fuzzing,Generators;Grammar;Informatics;Instruments;Java;Phosphors;Reverse engineering,Boolean algebra;context-free grammars;data flow analysis;data mining;program processors;reverse engineering;software tools,AUTOGRAM tool;Boolean value true;JSON value;context-free grammar;data flow;dynamic tainting;input grammar mining;program processes;program structure;reverse engineering;string value;syntactical entities identification;test generators,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
A Practical Guide to Select Quality Indicators for Assessing Pareto-Based Search Algorithms in Search-Based Software Engineering,S. Wang; S. Ali; T. Yue; Y. Li; M. Liaaen,"Simula Res. Lab., Oslo, Norway",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,631,642,"Many software engineering problems are multi-objective in nature, which has been largely recognized by the Search-based Software Engineering (SBSE) community. In this regard, Pareto- based search algorithms, e.g., Non-dominated Sorting Genetic Algorithm II, have already shown good performance for solving multi-objective optimization problems. These algorithms produce Pareto fronts, where each Pareto front consists of a set of non- dominated solutions. Eventually, a user selects one or more of the solutions from a Pareto front for their specific problems. A key challenge of applying Pareto-based search algorithms is to select appropriate quality indicators, e.g., hypervolume, to assess the quality of Pareto fronts. Based on the results of an extended literature review, we found that the current literature and practice in SBSE lacks a practical guide for selecting quality indicators despite a large number of published SBSE works. In this direction, the paper presents a practical guide for the SBSE community to select quality indicators for assessing Pareto-based search algorithms in different software engineering contexts. The practical guide is derived from the following complementary theoretical and empirical methods: 1) key theoretical foundations of quality indicators; 2) evidence from an extended literature review; and 3) evidence collected from an extensive experiment that was conducted to evaluate eight quality indicators from four different categories with six Pareto-based search algorithms using three real industrial problems from two diverse domains.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884880,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886940,Multi-objective Software Engineering Problems;Pareto-based Search Algorithms;Practical Guide;Quality Indicators,Bibliographies;Classification algorithms;Convergence;Search problems;Sociology;Software algorithms;Software engineering,Pareto optimisation;search problems;software quality,Pareto front;Pareto-based search algorithms;SBSE community;multiobjective optimization problems;nondominated sorting genetic algorithm II;quality indicators;search-based software engineering,,5,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
Search-Based Adaptation Planning Framework for Self-Adaptive Systems,L. Wang,"Sch. of Comput. Sci. & Technol., Xidian Univ., Xian, China",2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170703,2017,,,465,466,"Future-generation Self-Adaptive Systems (SASs) are required to adapt to the multiple, interrelated, and evolving changes. Current adaptation planning methods, which consider only one or two changes at a time and assume that changes are independent and the prioritization of them is static, need to be improved. Arguing that the adaptation planning is a search problem, this thesis highlights the feasibility and potential benefits of adopting Search-Based Optimization as an innovative planning method. A search-based adaptation planning framework is proposed to deal with these changes and make the best decisions for future-generation SASs.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.21,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965386,Search-based optimization;Search-based software engineering;Self-adaptation planning;Self-adaptive systems,Linear programming;Optimization;Planning;Real-time systems;Search problems;Software engineering;Uncertainty,optimisation;search problems;software engineering,SAS;future-generation self-adaptive systems;innovative planning method;search problem;search-based adaptation planning framework;search-based optimization,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
SDiC: Context-based retrieval in Eclipse,B. Antunes; J. Cordeiro; P. Gomes,"Centre for Informatics and Systems of the University of Coimbra, Coimbra, Portugal",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,1467,1468,"While working in an IDE, developers typically deal with a large number of different artifacts at the same time. The software development process requires that they repeatedly switch between different artifacts, which often depends on searching for these artifacts in the source code structure. We propose a tool that integrates context-based search and recommendation of source code artifacts in Eclipse. The artifacts are collected from the workspace of the developer and represented using ontologies. A context model of the developer is used to improve search and give recommendations of these artifacts, which are ranked according to their relevance to the developer. The tool was tested by a group of developers and the results show that contextual information has an important role in retrieving relevant information for developers.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227061,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227061,,Computational modeling;Context;Context modeling;Knowledge based systems;Ontologies;Programming;Switches,content-based retrieval;ontologies (artificial intelligence);recommender systems;software engineering,Eclipse;IDE;SDiC;context-based retrieval;context-based search;information retrieval;ontologies;software development in context;software development process;source code artifact recommendation;source code structure,,0,,4,,,,2-9 June 2012,,IEEE,IEEE Conferences
Automated Test Suite Generation for Time-Continuous Simulink Models,R. Matinnejad; S. Nejati; L. C. Briand; T. Bruckmann,"SnT Centre, Univ. of Luxembourg, Luxembourg, Luxembourg",2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),20170403,2016,,,595,606,"All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Interdisciplinary domains such as Cyber Physical Systems (CPSs) seek approaches that incorporate different modeling needs and usages. Specifically, the Simulink modeling platform greatly appeals to CPS engineers due to its seamless support for simulation and code generation. In this paper, we propose a test generation approach that is applicable to Simulink models built for both purposes of simulation and code generation. We define test inputs and outputs as signals that capture evolution of values over time. Our test generation approach is implemented as a meta-heuristic search algorithm and is guided to produce test outputs with diverse shapes according to our proposed notion of diversity. Our evaluation, performed on industrial and public domain models, demonstrates that: (1) In contrast to the existing tools for testing Simulink models that are only applicable to a subset of code generation models, our approach is applicable to both code generation and simulation Simulink models. (2) Our new notion of diversity for output signals outperforms random baseline testing and an existing notion of signal diversity in revealing faults in Simulink models. (3) The fault revealing ability of our test generation approach outperforms that of the Simulink Design Verifier, the only testing toolbox for Simulink.",,Electronic:978-1-4503-3900-1; POD:978-1-5090-2071-3,10.1145/2884781.2884797,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886937,Output diversity;Search-based software testing;Signal features;Simulink Design Verifier (SLDV);Simulink models;Software testing;Structural coverage;Time-continuous behaviors,Computational modeling;Fuels;Mathematical model;Shape;Software packages;Testing,automatic testing;program compilers;program testing;search problems,automated test suite generation;code generation;fault revealing ability;industrial domain models;meta-heuristic search algorithm;public domain models;signal diversity;simulation Simulink models;test inputs;test outputs;time-continuous Simulink models,,3,,,,,,14-22 May 2016,,IEEE,IEEE Conferences
Learning Combinatorial Interaction Test Generation Strategies Using Hyperheuristic Search,Y. Jia; M. B. Cohen; M. Harman; J. Petke,"Univ. Coll. London, London, UK",2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,1,,540,550,"The surge of search based software engineering research has been hampered by the need to develop customized search algorithms for different classes of the same problem. For instance, two decades of bespoke Combinatorial Interaction Testing (CIT) algorithm development, our exemplar problem, has left software engineers with a bewildering choice of CIT techniques, each specialized for a particular task. This paper proposes the use of a single hyperheuristic algorithm that learns search strategies across a broad range of problem instances, providing a single generalist approach. We have developed a Hyperheuristic algorithm for CIT, and report experiments that show that our algorithm competes with known best solutions across constrained and unconstrained problems: For all 26 real-world subjects, it equals or outperforms the best result previously reported in the literature. We also present evidence that our algorithm's strong generic performance results from its unsupervised learning. Hyperheuristic search is thus a promising way to relocate CIT design intelligence from human to machine.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.71,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194604,CIT;Hyperheuristic;SBSE,Algorithm design and analysis;Arrays;Heuristic algorithms;Search problems;Simulated annealing;Software algorithms;Testing,program testing;software engineering;unsupervised learning,CIT algorithm development;combinatorial interaction test generation strategies;hyperheuristic search strategies;search based software engineering;unsupervised learning,,8,,40,,,,16-24 May 2015,,IEEE,IEEE Conferences
CBCD: Cloned buggy code detector,J. Li; M. D. Ernst,"DNV Research & Innovation H&#x2298;vik, Norway",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,310,320,"Developers often copy, or clone, code in order to reuse or modify functionality. When they do so, they also clone any bugs in the original code. Or, different developers may independently make the same mistake. As one example of a bug, multiple products in a product line may use a component in a similar wrong way. This paper makes two contributions. First, it presents an empirical study of cloned buggy code. In a large industrial product line, about 4% of the bugs are duplicated across more than one product or file. In three open source projects (the Linux kernel, the Git version control system, and the PostgreSQL database) we found 282, 33, and 33 duplicated bugs, respectively. Second, this paper presents a tool, CBCD, that searches for code that is semantically identical to given buggy code. CBCD tests graph isomorphism over the Program Dependency Graph (PDG) representation and uses four optimizations. We evaluated CBCD by searching for known clones of buggy code segments in the three projects and compared the results with text-based, token-based, and AST-based code clone detectors, namely Simian, CCFinder, Deckard, and CloneDR. The evaluation shows that CBCD is fast when searching for possible clones of the buggy code in a large system, and it is more precise for this purpose than the other code clone detectors.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227183,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227183,Debugging aids;Validation,Cloning;Complexity theory;Computer bugs;Detectors;Kernel;Linux;Optimization,Linux;SQL;configuration management;graph theory;operating system kernels;program debugging;public domain software,AST-based code clone detectors;CBCD;CBCD tests graph isomorphism;CCFinder;CloneDR;Deckard;Git version control system;Linux kernel;PDG;PostgreSQL database;Simian;buggy code segments;cloned buggy code detector;industrial product line;open source projects;program dependency graph representation;text-based code clone detectors;token-based code clone detectors,,18,,34,,,,2-9 June 2012,,IEEE,IEEE Conferences
A Synergistic Approach for Distributed Symbolic Execution Using Test Ranges,Rui Qiu; S. Khurshid; C. S. Pasareanu; Guowei Yang,"Univ. of Texas, Austin, TX, USA",2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),20170824,2017,,,130,132,"Symbolic execution is a systematic program analysis technique that has received a lot of attention in the research community. However, scaling symbolic execution continues to pose a major challenge. This paper introduces Synergise, a novel two-fold integration approach. One, it integrates distributed analysis and constraint re-use to enhance symbolic execution using feasible ranges, which allow sharing of constraint solving results among different workers without communicating or sharing potentially large constraint databases (as required traditionally). Two, it integrates complementary techniques for test input generation, e.g., search-based generation and symbolic execution, for creating higher quality tests using unexplored ranges, which allows symbolic execution to re-use tests created by another technique for effective distribution of exploration of previously unexplored paths.",,Electronic:978-1-5386-1589-8; POD:978-1-5386-1590-4,10.1109/ICSE-C.2017.116,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965278,,Databases;Java;Software engineering;Standards;Systematics;Testing;Tools,constraint handling;distributed processing;program diagnostics;program testing;software reusability,SynergiSE approach;constraint databases;constraint reuse;distributed analysis;distributed symbolic execution;feasible ranges;search-based generation;synergistic approach;systematic program analysis;test input generation;test ranges;two-fold integration;unexplored ranges,,,,,,,,20-28 May 2017,,IEEE,IEEE Conferences
8th International Workshop on Search-Based Software Testing (SBST 2015),G. Gay; G. Antoniol,,2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,20150817,2015,2,,1001,1002,"This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.",0270-5257;02705257,Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2,10.1109/ICSE.2015.323,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148,,Conferences;Measurement;Search problems;Software;Software engineering;Software testing,,,,0,,3,,,,16-24 May 2015,,IEEE,IEEE Conferences
Towards automated testing and fixing of re-engineered Feature Models,C. Henard; M. Papadakis; G. Perrouin; J. Klein; Y. Le Traon,"Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg, Luxembourg",2013 35th International Conference on Software Engineering (ICSE),20130926,2013,,,1245,1248,"Mass customization of software products requires their efficient tailoring performed through combination of features. Such features and the constraints linking them can be represented by Feature Models (FMs), allowing formal analysis, derivation of specific variants and interactive configuration. Since they are seldom present in existing systems, techniques to re-engineer FMs have been proposed. There are nevertheless error-prone and require human intervention. This paper introduces an automated search-based process to test and fix FMs so that they adequately represent actual products. Preliminary evaluation on the Linux kernel FM exhibit erroneous FM constraints and significant reduction of the inconsistencies.",0270-5257;02705257,Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6,10.1109/ICSE.2013.6606689,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606689,Feature Model;Fixing;Search-based;Testing,Computational modeling;Context;Frequency modulation;Kernel;Linux;Testing,Linux;formal specification;operating system kernels;program testing;program verification,Linux kernel FM;automated search-based process;automated testing;erroneous FM constraint;feature combination;formal analysis;interactive configuration;mass customization;reengineered feature model;software product,,10,,21,,,,18-26 May 2013,,IEEE,IEEE Conferences
Sound empirical evidence in software testing,G. Fraser; A. Arcuri,"Saarland University, Saarbr&#x00FC;cken, Germany",2012 34th International Conference on Software Engineering (ICSE),20120628,2012,,,178,188,"Several promising techniques have been proposed to automate different tasks in software testing, such as test data generation for object-oriented software. However, reported studies in the literature only show the feasibility of the proposed techniques, because the choice of the employed artifacts in the case studies (e.g., software applications) is usually done in a non-systematic way. The chosen case study might be biased, and so it might not be a valid representative of the addressed type of software (e.g., internet applications and embedded systems). The common trend seems to be to accept this fact and get over it by simply discussing it in a threats to validity section. In this paper, we evaluate search-based software testing (in particular the EvoSuite tool) when applied to test data generation for open source projects. To achieve sound empirical results, we randomly selected 100 Java projects from SourceForge, which is the most popular open source repository (more than 300,000 projects with more than two million registered users). The resulting case study not only is very large (8,784 public classes for a total of 291,639 bytecode level branches), but more importantly it is statistically sound and representative for open source projects. Results show that while high coverage on commonly used types of classes is achievable, in practice environmental dependencies prohibit such high coverage, which clearly points out essential future research directions. To support this future research, our SF100 case study can serve as a much needed corpus of classes for test generation.",0270-5257;02705257,Electronic:978-1-4673-1067-3; POD:978-1-4673-1066-6,10.1109/ICSE.2012.6227195,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227195,class corpus;environment;search-based software engineering;security exception;test case generation;unit testing,Containers;Context;Java;Security;Software;Software testing,Java;object-oriented methods;program testing;public domain software,Java projects;SF100 case study;SourceForge;object-oriented software;open source projects;open source repository;search-based software testing;test data generation,,33,,54,,,,2-9 June 2012,,IEEE,IEEE Conferences
